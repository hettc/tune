{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "E0424 23:17:57.126233 140677271708736 program.py:300] TensorBoard could not bind to port 6005, it was already in use\n",
      "ERROR: TensorBoard could not bind to port 6005, it was already in use\n"
     ]
    }
   ],
   "source": [
    "# is this needed ?\n",
    "# import os\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = \"/workspace/.cache/huggingface/datasets\"\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/.cache/huggingface/models\"\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AdamW\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import util\n",
    "\n",
    "log_board = util.LogBoard(\"log_dir\", 6005)\n",
    "log_board.launch()\n",
    "\n",
    "log_every = 10\n",
    "step_every = 4\n",
    "save_every = 500\n",
    "bin = \"path/to/bin\"\n",
    "tokenizer_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998b95ef22fd4974888e60ce2499601f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ad4176f5b045d9ad5e4afa6bd1ae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcd0705d9e8443d92e8338b9645a3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11c933e9f0741ba984cc66438aa353d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ccc58a48c04a88ad2003dce10f0066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3e6c99a9574f2b8fb5a7f6e5b3635e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99077555e5ee47ee80b48fcd2ddc256c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a633bd72310e499e9f5aa79e39be4160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaDecoderLayer' object has no attribute 'attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tokenizer\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tuneable_hf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m train_tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m train_tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/code/work/tools/tune/src/util/model.py:86\u001b[0m, in \u001b[0;36mbuild_tuneable_hf_model\u001b[0;34m(untuned_model_id, quantized, device_map)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LoRA(layer, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 86\u001b[0m     layer\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mq_proj \u001b[38;5;241m=\u001b[39m get_adapter(\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[38;5;241m.\u001b[39mq_proj)\n\u001b[1;32m     87\u001b[0m     layer\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mk_proj \u001b[38;5;241m=\u001b[39m get_adapter(layer\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mk_proj)\n\u001b[1;32m     88\u001b[0m     layer\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m get_adapter(layer\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mv_proj)\n",
      "File \u001b[0;32m~/miniconda3/envs/py11/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaDecoderLayer' object has no attribute 'attn'"
     ]
    }
   ],
   "source": [
    "\n",
    "# model & tokenizer\n",
    "\n",
    "model = util.model.build_tuneable_hf_model(model_id, quantized=True, device_map='auto')\n",
    "train_tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, trust_remote_code=True)\n",
    "train_tokenizer.pad_token = train_tokenizer.eos_token\n",
    "train_tokenizer.padding_side = \"left\"\n",
    "train_tokenizer.add_bos_token = False\n",
    "train_tokenizer.add_eos_token = False\n",
    "train_tokenizer.__class__ = util.TrainingTokenizer\n",
    "train_tokenizer.max_seq = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset\n",
    "\n",
    "data_root = '../data'\n",
    "\n",
    "instruct_tune_dataset = load_dataset( 'json', data_files={\n",
    "    'train': f'{data_root}/train.jsonl', \n",
    "    'test': f'{data_root}/test.jsonl'\n",
    "}).map(lambda x: {\n",
    "    \"conversations\": util.convo.preprocess_sample(x['conversations'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finetune\n",
    "\n",
    "logger = log_board.get_logger(\"train\")\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_loader = DataLoader(instruct_tune_dataset['train'], batch_size=1, shuffle=True)\n",
    "def collator(batch):\n",
    "    return train_tokenizer.pad([*map(lambda x: train_tokenizer(x), batch['conversations'])], return_tensors=\"pt\", padding=\"max_length\", max_length=max_seq_length)\n",
    "\n",
    "\n",
    "i = 0\n",
    "model.train()\n",
    "for epoch in range(1000):  # number of epochs\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        i += 1\n",
    "\n",
    "        # prepare inputs\n",
    "        inputs = collator(batch)\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        padding = torch.full((labels.shape[0], 1), -100, dtype=torch.long, device=labels.device)  # Pad with -100 on the right\n",
    "        labels = torch.cat([labels, padding], dim=1)[:, 1:]\n",
    "\n",
    "        # forward backward\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        if i % step_every == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if i % log_every == 0:\n",
    "            logger.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            os.makedirs(bin, exist_ok=True)\n",
    "            util.models.save_model(model, save_path=f\"{bin}/model-tuned.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# upload to huggingface\n",
    "# REMEMBER to actually move model data into folder first\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    repo_id = \"host_name/model_name\",\n",
    "    folder_path = \"/path/to/model/and/parts\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
